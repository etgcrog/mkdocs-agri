{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"SoulCode Academy Documenta\u00e7\u00e3o do Projeto Final - Agricultura Turma BC8 - Engenharia de Dados Integrantes do grupo: Eduardo Teles | Joyce Meireles | Juliana Maciel | Sandro Gon\u00e7alves | Wesley Aranha Case do Projeto: O objetivo desse projeto \u00e9 reunir Datasets do tema agricultura e desenvolvimento rural do Brasil, realizar o processo de ETL e usar os dados para mostrar as vantagens da agricultura 4.0, realizando uma migra\u00e7\u00e3o de dados em um banco relacional (Postgre) para um banco n\u00e3o relacional (Cassandra), utilizando a plataforma em nuvem do Google (Googlo Cloud Platform - GCP) e a utiliza\u00e7\u00e3o da interface PySpark para a leitura dos registros, e com isso, extra\u00edr as informa\u00e7\u00f5es para analisar e obter insights que ajudem na tomada de decis\u00e3o de um poss\u00edvel projeto ou empresa. Etapas do projeto: Todo o projeto foi constru\u00eddo na Google Cloud Platform, seguindos as etapas abaixo: Escolha das tecnologias/ferramentas utilizadas Escolha dos Datasets Cria\u00e7\u00e3o do BD Postgre Tratamento inicial dos datasets e inser\u00e7\u00e3o no Postgre Sele\u00e7\u00e3o dos dados no Postgre e cria\u00e7\u00e3o dos Parquet Cria\u00e7\u00e3o do BD Cassandra Inser\u00e7\u00e3o dos Parquet no Cassandra An\u00e1lise e cria\u00e7\u00e3o de gr\u00e1ficos Documenta\u00e7\u00e3o 1- Tecnologias/Ferramentas utilizadas no projeto: O Banco de dados relacional utilizado foi o Postgre ; Vantagens do Postgre sobre o MySQL Foi escolhido o Postgre por sua escalabilidade e robustez,suporta bases de dados grandes, complexas com tamanhos ilimitados de linhas, bancos de dados e tabelas (at\u00e9 16TB), aceita v\u00e1rios tipos de sub-consultas, possui mais tipos de dados e conta com um bom mecanismo de FAILSAVE (Seguran\u00e7a contra falhas, por exemplo no desligamento repentino do sistema), e garante escalabilidade e confian\u00e7a. Fonte da informa\u00e7\u00e3o (clique aqui) O Banco de dados n\u00e3o relacional utilizado foi o Cassandra ; Vantagens do Cassandra sobre o MongoDB Se voc\u00ea trabalha com uma grande quantidade de dados e precisa de velocidade para armazenar esses dados, voc\u00ea pode escolher o Cassandra. Se voc\u00ea precisa de uma r\u00e1pida leitura dos dados, os dois bancos s\u00e3o ideais para voc\u00ea. Se voc\u00ea j\u00e1 tem conhecimentos em SQL e quer continuar a utilizar uma linguagem bem semelhante, voc\u00ea pode escolher o Cassandra. Se voc\u00ea vai trabalhar com uma grande quantidade de dados e precisa de uma grande velocidade de grava\u00e7\u00e3o e leitura, voc\u00ea pode escolher o Cassandra. Fonte da informa\u00e7\u00e3o (clique aqui) O m\u00f3dulo utilizado para a conex\u00e3o com o Postgre foi o Psycopg2 ; A API JDBC foi utilizada para a conex\u00e3o com o Postgre e Cassandra; A JDBC (Java Database Connectivity), faz o envio de instru\u00e7\u00f5es SQL para qualquer banco de dados relacional, desde que haja um driver que corresponda ao mesmo presente. Outra das vantagens da JDBC \u00e9 o fato dela funcionar como uma camada de abstra\u00e7\u00e3o de dados. Independente do SGBD utilizado, a API ser\u00e1 a mesma, facilitando muito a vida dos programadores caso haja a necessidade de uma migra\u00e7\u00e3o de banco. Fonte da informa\u00e7\u00e3o (clique aqui) O m\u00f3dulo do Pandas no Python foi utilizado para o tratamento dos datasets; A interface Pyspark foi utilizada para criar os parquet e tratar os dados; O Apache Spark \u00e9 uma engine de computa\u00e7\u00e3o unificada e um conjunto de bibliotecas para processamento de dados paralelos em clusters de computadores. Uma das grandes vantagens do Spark \u00e9 ser capaz de trabalhar de forma distribu\u00edda. Isso significa que se tratando de conjuntos de dados muito grandes ou quando a entrada de novos dados acontece de forma muito r\u00e1pida, pode se tornar demais para um \u00fanico computador. \u00c9 aqui que entra a computa\u00e7\u00e3o distribu\u00edda. Em vez de tentar processar um enorme conjunto de dados, essas tarefas s\u00e3o divididas entre diversas m\u00e1quinas que est\u00e3o em constante comunica\u00e7\u00e3o entre si. Em um sistema de computa\u00e7\u00e3o distribu\u00eddo, cada computador individual \u00e9 chamado de n\u00f3 (node) e a cole\u00e7\u00e3o de todos eles \u00e9 chamada de cluster. Muito da popularidade do Spark se deve aos fatores listados abaixo: - Suporta diferentes linguagens de programa\u00e7\u00e3o como Java, Python, Scala e R. - Possibilita streaming de dados em tempo real. - \u00c9 poss\u00edvel rodar em uma \u00fanica m\u00e1quina assim como em grandes clusters de computadores. - Suporta SQL. - Possui bibliotecas para criar aplica\u00e7\u00f5es com Machine Learning, MLlib. - Realiza processamento de dados em grafos com GraphX. - \u00c9 uma ferramenta open-source. Fonte da informa\u00e7\u00e3o (clique aqui) Porque utilizar o formato parquet? Usamos Parquet que \u00e9 um armazenamento em colunas em geral - Para armazenar dados por colunas para otimizar o desempenho de consultas anal\u00edticas e diminuir o custo de armazenamento. Fonte da informa\u00e7\u00e3o (clique aqui) As ferramentas utilizadas na GCP foram: Cloud Storage, Dataproc, Postgre, Cassandra(Google click to deploy), Compute Engine, IDE Jupyter ; Utilizamos o m\u00f3dulo Matplotlib e HTML para a cria\u00e7\u00e3o dos gr\u00e1ficos e an\u00e1lises; Utilizamos Markdown para a cria\u00e7\u00e3o da documenta\u00e7\u00e3o; A apresenta\u00e7\u00e3o foi feita no Canvas . 2- Escolha dos Datasets: Ao todo foram encontrados e escolhidos os seguintes Datasets: Exporta\u00e7\u00e3o X Pa\u00edses ( AGROSTAT ) - Dados: 227 Exporta\u00e7\u00e3o X Categoria de produtos ( AGROSTAT ) - Dados: 26 Exporta\u00e7\u00e3o X UF ( AGROSTAT ) - Dados: 33 PIB Brasil ( CEPEA-PIB Brasil ) - Dados: 25 U.S. Department of Agriculture ( USDA ) - Dados: 112 Value of Agricultural Production ( FAOSTAT ) - Dados: 3.213.519 Crops and livestock products ( FAOSTAT ) - Dados: 3.807.009 Total de dados: 7.020.951 3- Cria\u00e7\u00e3o do BD Postgre: Constru\u00edmos o BD Postgre para ter 8 tabelas e 7 trigger functions que receberam os dados dos datasets. O Diagrama Entidade Relacionamento do BD Postgre se encotra a seguir: As Trigger Functions foram utilizadas para realizarem determinados tratamentos nos dados quando inseridos no Banco. 1- Exemplo de uma das Trigger Functions que foram utilizadas: CREATE OR REPLACE FUNCTION alterar_valor() RETURNS trigger AS $$ BEGIN UPDATE pib_agricola SET insumo = (New.insumo * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET agropecuaria = (New.agropecuaria * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET industria = (New.industria * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET servicos = (New.servicos * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET total = (New.total * 1000000) WHERE id_pib = New.id_pib; RETURN NEW; END; $$ LANGUAGE 'plpgsql'; CREATE TRIGGER tgr_pib_agicola_update AFTER INSERT ON pib_agricola FOR EACH ROW EXECUTE PROCEDURE alterar_valor(); 4- Tratamento inicial dos datasets e inser\u00e7\u00e3o no Postgre: Os tratamentos dos datasets foram realizados com o m\u00f3dulo do pandas do Python e com as Trigger Functions criadas no BD Postgre. Alguns dos exemplos de tratamentos com o pandas que utilizamos: 1- Exclus\u00e3o de colunas: df.drop(columns =['Area Code', 'Item Code', 'Element Code','Year Code', 'Flag'], axis =1, inplace = True) 2- Transposto de um DataFrame e a redefini\u00e7\u00e3o do \u00edndice do mesmo: pais = pais.T.reset_index() 3- Preenchimento dos valores vazios com o valor \"0\": pais = pais.fillna(0) Para a inser\u00e7\u00e3o no Postgre, foi utilizado o m\u00f3dulo psycopg2 e criamos a Classe Connectar com os dois m\u00e9todos de inser\u00e7\u00e3o, listados abaixo: 1- M\u00e9todo utilizado para a inser\u00e7\u00e3o individual de valores: def inserir(self, table, parametros, valores): query = f\"INSERT INTO {table} ({parametros}) VALUES ({valores})\" self.acao(query) 2- M\u00e9todo utilizado para a inser\u00e7\u00e3o de m\u00faltiplos valores: def inserir_multiples(self, table, parametros, valores): query = f\"INSERT INTO {table} ({parametros}) VALUES {valores}\" self.acao(query) Desafio encontrado nessa etapa do projeto: No momento da inser\u00e7\u00e3o do maior CSV, com mais de 3 milh\u00f5es de dados, tivemos que realizar essa inser\u00e7\u00e3o em blocos de dados e a nossa solu\u00e7\u00e3o \u00e9 apresentada no c\u00f3digo abaixo: try: df3 = c1.ler_csv('gs://desafio-agricultura/CSV/quantidade(FAO)_final.csv') df3.drop(\"Unnamed: 0\", axis=1, inplace=True) df3 = df3.fillna(0) tam = df3.shape[0] k = 0 j = 500000 while True: arr = np.array(df3) valores = [] if j > 3000000: j = 3000000 + (tam - 3000000) for i in arr[k:j]: item = (i[0], i[1], i[2], i[3], i[4], i[5]) valores.append(item) valores = str(valores)[1:-1] # print(lista_antes) c1.inserir_multiples('quantidade_colheita','pais, item, elemento, ano_quantidade, unidade, valor_quantidade', valores) if j == tam: break j += 500000 k += 500000 except Exception as e: print(e) 5- Sele\u00e7\u00e3o dos dados no Postgre e cria\u00e7\u00e3o dos Parquet: Para a sele\u00e7\u00e3o dos dados presentes no Postgre, utilizamos a API JDBC e a interface Pyspark que nos permitiu usar o Spark por meio da linguagem Python. Em uma mesma fun\u00e7\u00e3o(mostrada abaixo) conseguimos ler as informa\u00e7\u00f5es e transform\u00e1-las em Parquet. 1- M\u00e9todo utilizado para selecionar os dados no BD Postgre e gerar os parquet: def read_transform_parquet(table_name, path_parquet): url = 'jdbc:postgresql://34.95.140.217:5432/desafio_final' properties = { 'user': 'postgres', 'password': 'root', 'driver': 'org.postgresql.Driver' } df = spark.read.jdbc(url=url, table=table_name, properties=properties) df.write.parquet(path_parquet) return f\"Parquet add in {path_parquet}\" 6- Cria\u00e7\u00e3o do BD Cassandra: No BD Cassandra foram criadas 8 tabelas que receberam os dados presentes no Postgre. Abaixo segue o c\u00f3digo da cria\u00e7\u00e3o da Keyspace e de uma das tabelas criadas: 1- C\u00f3digo da cria\u00e7\u00e3o do Keyspace: create keyspace if not exists desafio_agricultura; with replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; 2- C\u00f3digo da cria\u00e7\u00e3o da tabela do PIB agr\u00edcola Brasileiro: CREATE TABLE IF NOT EXISTS \"desafio_agricultura\".\"pib_agricola\" ( id_pib int primary key, ano_pib text, insumo float, agropecuaria float, industria float, servicos float, total float ); 7- Inser\u00e7\u00e3o dos Dados (parquet) no Cassandra: Para a inser\u00e7\u00e3o dos dados no BD Cassandra, utilizamos a API JDBC para realizar a conex\u00e3o com o Banco e Pyspark para leitura e tratamento dos dados. 1- C\u00f3digo que realiza a conex\u00e3o com o Banco de Dados Cassandra: spark = SparkSession\\ .builder\\ .appName(\"Spark Exploration App\")\\ .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.1.0\")\\ .config(\"spark.sql.extensions\",\"com.datastax.spark.connector.CassandraSparkExtensions\") \\ .config(\"spark.cassandra.connection.host\",\"172.17.0.4\") \\ .config(\"spark.cassandra.connection.port\",\"9042\") \\ .getOrCreate() 2 - M\u00e9todo que realiza a inser\u00e7\u00e3o no Cassandra: def saveData(df, table): df.write \\ .format(\"org.apache.spark.sql.cassandra\") \\ .option(\"keyspace\", keyspace) \\ .option(\"table\", table) \\ .mode('append') \\ .save() 8- An\u00e1lise e cria\u00e7\u00e3o de gr\u00e1ficos: Para as an\u00e1lises e a cria\u00e7\u00e3o dos gr\u00e1ficos, utilizamos o m\u00f3dulo Matplotlib presente no Python e Google Charts/JavaScript. 9- Documenta\u00e7\u00e3o: Nossa documenta\u00e7\u00e3o foi feita em Markdown.","title":"Home"},{"location":"#soulcode-academy","text":"","title":"SoulCode Academy"},{"location":"#documentacao-do-projeto-final-agricultura","text":"Turma BC8 - Engenharia de Dados","title":"Documenta\u00e7\u00e3o do Projeto Final - Agricultura"},{"location":"#integrantes-do-grupo","text":"Eduardo Teles | Joyce Meireles | Juliana Maciel | Sandro Gon\u00e7alves | Wesley Aranha","title":"Integrantes do grupo:"},{"location":"#case-do-projeto","text":"O objetivo desse projeto \u00e9 reunir Datasets do tema agricultura e desenvolvimento rural do Brasil, realizar o processo de ETL e usar os dados para mostrar as vantagens da agricultura 4.0, realizando uma migra\u00e7\u00e3o de dados em um banco relacional (Postgre) para um banco n\u00e3o relacional (Cassandra), utilizando a plataforma em nuvem do Google (Googlo Cloud Platform - GCP) e a utiliza\u00e7\u00e3o da interface PySpark para a leitura dos registros, e com isso, extra\u00edr as informa\u00e7\u00f5es para analisar e obter insights que ajudem na tomada de decis\u00e3o de um poss\u00edvel projeto ou empresa.","title":"Case do Projeto:"},{"location":"#etapas-do-projeto","text":"Todo o projeto foi constru\u00eddo na Google Cloud Platform, seguindos as etapas abaixo: Escolha das tecnologias/ferramentas utilizadas Escolha dos Datasets Cria\u00e7\u00e3o do BD Postgre Tratamento inicial dos datasets e inser\u00e7\u00e3o no Postgre Sele\u00e7\u00e3o dos dados no Postgre e cria\u00e7\u00e3o dos Parquet Cria\u00e7\u00e3o do BD Cassandra Inser\u00e7\u00e3o dos Parquet no Cassandra An\u00e1lise e cria\u00e7\u00e3o de gr\u00e1ficos Documenta\u00e7\u00e3o","title":"Etapas do projeto:"},{"location":"#1-tecnologiasferramentas-utilizadas-no-projeto","text":"O Banco de dados relacional utilizado foi o Postgre ; Vantagens do Postgre sobre o MySQL Foi escolhido o Postgre por sua escalabilidade e robustez,suporta bases de dados grandes, complexas com tamanhos ilimitados de linhas, bancos de dados e tabelas (at\u00e9 16TB), aceita v\u00e1rios tipos de sub-consultas, possui mais tipos de dados e conta com um bom mecanismo de FAILSAVE (Seguran\u00e7a contra falhas, por exemplo no desligamento repentino do sistema), e garante escalabilidade e confian\u00e7a. Fonte da informa\u00e7\u00e3o (clique aqui) O Banco de dados n\u00e3o relacional utilizado foi o Cassandra ; Vantagens do Cassandra sobre o MongoDB Se voc\u00ea trabalha com uma grande quantidade de dados e precisa de velocidade para armazenar esses dados, voc\u00ea pode escolher o Cassandra. Se voc\u00ea precisa de uma r\u00e1pida leitura dos dados, os dois bancos s\u00e3o ideais para voc\u00ea. Se voc\u00ea j\u00e1 tem conhecimentos em SQL e quer continuar a utilizar uma linguagem bem semelhante, voc\u00ea pode escolher o Cassandra. Se voc\u00ea vai trabalhar com uma grande quantidade de dados e precisa de uma grande velocidade de grava\u00e7\u00e3o e leitura, voc\u00ea pode escolher o Cassandra. Fonte da informa\u00e7\u00e3o (clique aqui) O m\u00f3dulo utilizado para a conex\u00e3o com o Postgre foi o Psycopg2 ; A API JDBC foi utilizada para a conex\u00e3o com o Postgre e Cassandra; A JDBC (Java Database Connectivity), faz o envio de instru\u00e7\u00f5es SQL para qualquer banco de dados relacional, desde que haja um driver que corresponda ao mesmo presente. Outra das vantagens da JDBC \u00e9 o fato dela funcionar como uma camada de abstra\u00e7\u00e3o de dados. Independente do SGBD utilizado, a API ser\u00e1 a mesma, facilitando muito a vida dos programadores caso haja a necessidade de uma migra\u00e7\u00e3o de banco. Fonte da informa\u00e7\u00e3o (clique aqui) O m\u00f3dulo do Pandas no Python foi utilizado para o tratamento dos datasets; A interface Pyspark foi utilizada para criar os parquet e tratar os dados; O Apache Spark \u00e9 uma engine de computa\u00e7\u00e3o unificada e um conjunto de bibliotecas para processamento de dados paralelos em clusters de computadores. Uma das grandes vantagens do Spark \u00e9 ser capaz de trabalhar de forma distribu\u00edda. Isso significa que se tratando de conjuntos de dados muito grandes ou quando a entrada de novos dados acontece de forma muito r\u00e1pida, pode se tornar demais para um \u00fanico computador. \u00c9 aqui que entra a computa\u00e7\u00e3o distribu\u00edda. Em vez de tentar processar um enorme conjunto de dados, essas tarefas s\u00e3o divididas entre diversas m\u00e1quinas que est\u00e3o em constante comunica\u00e7\u00e3o entre si. Em um sistema de computa\u00e7\u00e3o distribu\u00eddo, cada computador individual \u00e9 chamado de n\u00f3 (node) e a cole\u00e7\u00e3o de todos eles \u00e9 chamada de cluster. Muito da popularidade do Spark se deve aos fatores listados abaixo: - Suporta diferentes linguagens de programa\u00e7\u00e3o como Java, Python, Scala e R. - Possibilita streaming de dados em tempo real. - \u00c9 poss\u00edvel rodar em uma \u00fanica m\u00e1quina assim como em grandes clusters de computadores. - Suporta SQL. - Possui bibliotecas para criar aplica\u00e7\u00f5es com Machine Learning, MLlib. - Realiza processamento de dados em grafos com GraphX. - \u00c9 uma ferramenta open-source. Fonte da informa\u00e7\u00e3o (clique aqui) Porque utilizar o formato parquet? Usamos Parquet que \u00e9 um armazenamento em colunas em geral - Para armazenar dados por colunas para otimizar o desempenho de consultas anal\u00edticas e diminuir o custo de armazenamento. Fonte da informa\u00e7\u00e3o (clique aqui) As ferramentas utilizadas na GCP foram: Cloud Storage, Dataproc, Postgre, Cassandra(Google click to deploy), Compute Engine, IDE Jupyter ; Utilizamos o m\u00f3dulo Matplotlib e HTML para a cria\u00e7\u00e3o dos gr\u00e1ficos e an\u00e1lises; Utilizamos Markdown para a cria\u00e7\u00e3o da documenta\u00e7\u00e3o; A apresenta\u00e7\u00e3o foi feita no Canvas .","title":"1- Tecnologias/Ferramentas utilizadas no projeto:"},{"location":"#2-escolha-dos-datasets","text":"Ao todo foram encontrados e escolhidos os seguintes Datasets: Exporta\u00e7\u00e3o X Pa\u00edses ( AGROSTAT ) - Dados: 227 Exporta\u00e7\u00e3o X Categoria de produtos ( AGROSTAT ) - Dados: 26 Exporta\u00e7\u00e3o X UF ( AGROSTAT ) - Dados: 33 PIB Brasil ( CEPEA-PIB Brasil ) - Dados: 25 U.S. Department of Agriculture ( USDA ) - Dados: 112 Value of Agricultural Production ( FAOSTAT ) - Dados: 3.213.519 Crops and livestock products ( FAOSTAT ) - Dados: 3.807.009 Total de dados: 7.020.951","title":"2- Escolha dos Datasets:"},{"location":"#3-criacao-do-bd-postgre","text":"Constru\u00edmos o BD Postgre para ter 8 tabelas e 7 trigger functions que receberam os dados dos datasets. O Diagrama Entidade Relacionamento do BD Postgre se encotra a seguir: As Trigger Functions foram utilizadas para realizarem determinados tratamentos nos dados quando inseridos no Banco. 1- Exemplo de uma das Trigger Functions que foram utilizadas: CREATE OR REPLACE FUNCTION alterar_valor() RETURNS trigger AS $$ BEGIN UPDATE pib_agricola SET insumo = (New.insumo * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET agropecuaria = (New.agropecuaria * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET industria = (New.industria * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET servicos = (New.servicos * 1000000) WHERE id_pib = New.id_pib; UPDATE pib_agricola SET total = (New.total * 1000000) WHERE id_pib = New.id_pib; RETURN NEW; END; $$ LANGUAGE 'plpgsql'; CREATE TRIGGER tgr_pib_agicola_update AFTER INSERT ON pib_agricola FOR EACH ROW EXECUTE PROCEDURE alterar_valor();","title":"3- Cria\u00e7\u00e3o do BD Postgre:"},{"location":"#4-tratamento-inicial-dos-datasets-e-insercao-no-postgre","text":"Os tratamentos dos datasets foram realizados com o m\u00f3dulo do pandas do Python e com as Trigger Functions criadas no BD Postgre. Alguns dos exemplos de tratamentos com o pandas que utilizamos: 1- Exclus\u00e3o de colunas: df.drop(columns =['Area Code', 'Item Code', 'Element Code','Year Code', 'Flag'], axis =1, inplace = True) 2- Transposto de um DataFrame e a redefini\u00e7\u00e3o do \u00edndice do mesmo: pais = pais.T.reset_index() 3- Preenchimento dos valores vazios com o valor \"0\": pais = pais.fillna(0) Para a inser\u00e7\u00e3o no Postgre, foi utilizado o m\u00f3dulo psycopg2 e criamos a Classe Connectar com os dois m\u00e9todos de inser\u00e7\u00e3o, listados abaixo: 1- M\u00e9todo utilizado para a inser\u00e7\u00e3o individual de valores: def inserir(self, table, parametros, valores): query = f\"INSERT INTO {table} ({parametros}) VALUES ({valores})\" self.acao(query) 2- M\u00e9todo utilizado para a inser\u00e7\u00e3o de m\u00faltiplos valores: def inserir_multiples(self, table, parametros, valores): query = f\"INSERT INTO {table} ({parametros}) VALUES {valores}\" self.acao(query) Desafio encontrado nessa etapa do projeto: No momento da inser\u00e7\u00e3o do maior CSV, com mais de 3 milh\u00f5es de dados, tivemos que realizar essa inser\u00e7\u00e3o em blocos de dados e a nossa solu\u00e7\u00e3o \u00e9 apresentada no c\u00f3digo abaixo: try: df3 = c1.ler_csv('gs://desafio-agricultura/CSV/quantidade(FAO)_final.csv') df3.drop(\"Unnamed: 0\", axis=1, inplace=True) df3 = df3.fillna(0) tam = df3.shape[0] k = 0 j = 500000 while True: arr = np.array(df3) valores = [] if j > 3000000: j = 3000000 + (tam - 3000000) for i in arr[k:j]: item = (i[0], i[1], i[2], i[3], i[4], i[5]) valores.append(item) valores = str(valores)[1:-1] # print(lista_antes) c1.inserir_multiples('quantidade_colheita','pais, item, elemento, ano_quantidade, unidade, valor_quantidade', valores) if j == tam: break j += 500000 k += 500000 except Exception as e: print(e)","title":"4- Tratamento inicial dos datasets e inser\u00e7\u00e3o no Postgre:"},{"location":"#5-selecao-dos-dados-no-postgre-e-criacao-dos-parquet","text":"Para a sele\u00e7\u00e3o dos dados presentes no Postgre, utilizamos a API JDBC e a interface Pyspark que nos permitiu usar o Spark por meio da linguagem Python. Em uma mesma fun\u00e7\u00e3o(mostrada abaixo) conseguimos ler as informa\u00e7\u00f5es e transform\u00e1-las em Parquet. 1- M\u00e9todo utilizado para selecionar os dados no BD Postgre e gerar os parquet: def read_transform_parquet(table_name, path_parquet): url = 'jdbc:postgresql://34.95.140.217:5432/desafio_final' properties = { 'user': 'postgres', 'password': 'root', 'driver': 'org.postgresql.Driver' } df = spark.read.jdbc(url=url, table=table_name, properties=properties) df.write.parquet(path_parquet) return f\"Parquet add in {path_parquet}\"","title":"5- Sele\u00e7\u00e3o dos dados no Postgre e cria\u00e7\u00e3o dos Parquet:"},{"location":"#6-criacao-do-bd-cassandra","text":"No BD Cassandra foram criadas 8 tabelas que receberam os dados presentes no Postgre. Abaixo segue o c\u00f3digo da cria\u00e7\u00e3o da Keyspace e de uma das tabelas criadas: 1- C\u00f3digo da cria\u00e7\u00e3o do Keyspace: create keyspace if not exists desafio_agricultura; with replication = {'class': 'SimpleStrategy', 'replication_factor': 1}; 2- C\u00f3digo da cria\u00e7\u00e3o da tabela do PIB agr\u00edcola Brasileiro: CREATE TABLE IF NOT EXISTS \"desafio_agricultura\".\"pib_agricola\" ( id_pib int primary key, ano_pib text, insumo float, agropecuaria float, industria float, servicos float, total float );","title":"6- Cria\u00e7\u00e3o do BD Cassandra:"},{"location":"#7-insercao-dos-dados-parquet-no-cassandra","text":"Para a inser\u00e7\u00e3o dos dados no BD Cassandra, utilizamos a API JDBC para realizar a conex\u00e3o com o Banco e Pyspark para leitura e tratamento dos dados. 1- C\u00f3digo que realiza a conex\u00e3o com o Banco de Dados Cassandra: spark = SparkSession\\ .builder\\ .appName(\"Spark Exploration App\")\\ .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.1.0\")\\ .config(\"spark.sql.extensions\",\"com.datastax.spark.connector.CassandraSparkExtensions\") \\ .config(\"spark.cassandra.connection.host\",\"172.17.0.4\") \\ .config(\"spark.cassandra.connection.port\",\"9042\") \\ .getOrCreate() 2 - M\u00e9todo que realiza a inser\u00e7\u00e3o no Cassandra: def saveData(df, table): df.write \\ .format(\"org.apache.spark.sql.cassandra\") \\ .option(\"keyspace\", keyspace) \\ .option(\"table\", table) \\ .mode('append') \\ .save()","title":"7- Inser\u00e7\u00e3o dos Dados (parquet) no Cassandra:"},{"location":"#8-analise-e-criacao-de-graficos","text":"Para as an\u00e1lises e a cria\u00e7\u00e3o dos gr\u00e1ficos, utilizamos o m\u00f3dulo Matplotlib presente no Python e Google Charts/JavaScript.","title":"8- An\u00e1lise e cria\u00e7\u00e3o de gr\u00e1ficos:"},{"location":"#9-documentacao","text":"Nossa documenta\u00e7\u00e3o foi feita em Markdown.","title":"9- Documenta\u00e7\u00e3o:"}]}